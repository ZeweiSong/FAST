# FAST Pipeline for processing next generation amplicon sequencing data.

# You can also use this pipeline under any Linux system.
# For MSI user: this is a pipeline that is compatible with the University of Minnesota MSI's mesabi system.
# The backbone of this piepline is written in Python, so there is no need for any installation. But you will need to install several software for certain functions.

# SECTION-0 PREPARATION ##########################################################################################################################################################

# SECTION-0-1 FAST package
# Download the FAST package (https://github.com/ZeweiSong/FAST) and put it into your working folder (For example ~/bin/FAST).
# You can then call the main script by using: python ~/bin/FAST/fast.py following by different parameters (You'll need to install Python first, both 2.7 and 3.3 work).
# For MSI user, you can put the FAST package in the shared folder, and make it accessible to all the members. The FAST package can be accessed by using a short cut in mesabi.

# SECTION-0-2 Denpendencies (Software or program you need besides the FAST package) 

# 1) cutadapt: primers/adaptors trimming
# Follow the installation instruction in the cutadapt webpage (https://cutadapt.readthedocs.org/en/stable/index.html).
# Usually the command is: pip install --user --upgrade cutadapt
# Cutadapt can then be called using: ~/.local/bin/cutadapt --help
# You can also copy the executive file to your working folder, or add the path to your environment.

# 2) VSEARCH: quality filter, pair-end merge, chimera checking, and OTU clustering
# VSEARCH is the open source version of the more popular USEARCH, which is not open source and have a 4 GB memory limitatino for its free version.
# There is paper on PeerJ (https://peerj.com/articles/1487.pdf) showing that VSEARCH in most of case is as good as or better than USEARCH at de vono OTU picking.
# Download and unzip the latest release of VSEARCH here: https://github.com/torognes/vsearch/releases
# The executive file "vsearch" is the only thing you'll need. Copy it to your working folder and add the path to your Linux environment.

# If you are using MSI, you should be able to load cutadapt as a module. But you'll need to upload VSEARCH executive by yourself.

# In my previous pipeline, I used Trimmomatic for primer/adaptor trimmig and quality filtering, but it truns out cutadapt is better and easier to use.

# For MSI user, start your .sh file with the following head:

#!/bin/bash -l 
#PBS -l walltime=2:00:00,nodes=2:ppn=8,pmem=2580mb # Rememeber to set the right walltime, I ususally separately the commandlines into several sections so you can check the progress.
#PBS -m abe 
#PBS -M songx208@umn.edu 
module load python-epd
module load cutadapt
cd /home/kinkelll/shared/Fungi # This is the path of your working folder.
FAST='/home/kinkelll/shared/FAST' # This is the path to the FAST package folder.

# If you are using Linux on a personal computer, you can still use a batch file for processing, but just remember to remove the header for MSI.

# After this head file, you can include any of the following command line. You can use the hasg tag "#" to skip any command line that have already been ran.

# SECTION-1 QUALITY CONTROL ##########################################################################################################################################################
# Your data should arrive as two FASTQ files per sample. For fungal data, copy all R1 read files to a new folder.
# For the example here, I'll put all R1 read files into /home/kinkelll/shared/Fungi/Read1

# Generate a mapping file for your R1 files
# You are going to repeatly use the following style commandline. "$FAST/fast.py" refer to the main script of the FAST package, it will summon all the functions.
# You can also use "-h" to obtain a help document, for example: python $FAST/fast.py -generate_mapping -h from the MSI Mesabi system.
python $FAST/fast.py -generate_mapping -i read1
# For personal computer, this command is equal to:
python bin/FAST/fast.py -generate_mapping -i read1
# I will use the MSI command as example in the file.

# Take a look at the mapping file. You should not have "_" (underscore) in the first column (Sample_Id). "_" is used for identify sample name in the script (it is also the style Qiime uses).

# Add sequence labels to all files, and save them to a new folder
# -t specify the thread/CPU you would like to use, for the resource we request, we can use 8 cpu at a same time (for desktop intel i5 and i7 usually have 4 CPUs).
python $FAST/fast.py -add_labels -m mapping.txt -i Read1 -o Read1_labeled -t 8

# Merge all labeled sequences into a single FASTQ file. # If you are using a cloud drive service, make sure you don't have any hidden file in this folder, otherwise they will also be included.
python $FAST/fast.py -merge_seqs -i Read1_labeled -o raw.fastq

# Use cutadapt to remove PCR primers
cutadapt raw.fastq -a GCATCGATGAAGAACGCAGC -g CTTGGTCATTTAGAGGAAGTAA -o raw.cut.fastq --time=2

# The 3' and 5' primers are specified using the -a and -g option, respectively. Remember to ask your sequencing facilities for the right primer. The 3' primer need to be reverse compliment (same thing appied to R2 read). The primers used here as example are ITS1F and ITS2 (amplify the ITS1 region).

# Filer sequences by max number of expected errors.
# Get a report on your FASTQ file
./vsearch --fastq_stats raw.cut.fastq --log raw.cut.log

# Based on the report, decide the maxEE parameters
./vsearch --fastq_filter raw.cut.fastq --fastq_maxee 1 --fastaout raw.cut.trim.fasta --fasta_width 0

# Get rid of sequences with ambiguous base and homopolyers longer than 9 bases (not include 9).
python $FAST/filter_seqs.py -i raw.cut.trim.fasta -o raw.cut.trim.N0.homop9.fasta -maxN 0 -maxhomop 9

# Get general information of the filered data.
# You can skip this one, but taking look at the report will help you decide the following trimming plan. It could also be a useful supplement for manuscript.
python $FAST/stat_seqs.py -i raw.trim.N0.homop9.fasta -o qc_report.txt

# Truncate all sequences to a fixed length, convert FASTQ to FASTA
# You can also skip this one, but this is recommend by the USEARCH author to truncate all sequences to a fixed length, if the read does not span the entire gene.
# For example, I truncate all sequences to a length of 200bp, which retains ~80% of my total sequences, by reading the qc_report.txt from the last step.
python $FAST/truncate_seqs.py -i raw.cut.trim.N0.homop9.fasta -l 200 -o raw.qc.L200.fasta


# SECTION-2 OTU CLUSTERING ################################################################################################################################################################
# Dereplication
# For some reason, using only one CPU is the fastest way in mesabi, so just use "-t 1" for MSI system.
# The output will be two files:
# 	raw.qc.derep.txt is the QIIME style OTU mapping.
#	raw.qc.derep.fasta is the dereplicated FASTA file.
python $FAST/dereplicate.py -i raw.trim.qc.L200.fasta -o raw.qc.derep -t 1

# Remove all singletons using the OTU map file.
python $FAST/filter_otu_map.py -i raw.qc.derep.txt -o raw.qc.derep.size2.txt -min_size 2

# Using the new OTU map file (without singletons) to pick sequences in the original dereplicated FASTA file (So you have a new set of mathced OTU map file and FASTA file).
# Also noticed that a size label (;size=XXX) is added to each sequence for VSEARCH.
python $FAST/fast.py -pick_seqs -i raw.qc.derep.fasta -o raw.qc.derep.size2.fasta -map raw.qc.derep.size2.txt -sizeout

# This is the step you can stop and plug into your own QIIME pipeline if you want. The OTU map and FASTA file are compatibale with all uses in QIIME.
# If you want to use VSEARCH to do clustering, keep going.

# Use VSEARCH to do OTU clustering
# By this time, the "raw.qc.derep.size2.fasta" file should be smaller, even for a very large dataset. For three full runs of MiSeq, I got a file around 200MB.

# De novo Chimera checking
./vsearch --uchime_denovo raw.qc.derep.size2.fasta --nonchimeras raw.qc.derep.size2.uchime.fasta --sizeout --fasta_width 0

# Using the single-pass, greedy star-clustering method in VSEARCH.
# We will need the UC format output to generate a Qiime style OTU table.
./vsearch raw.qc.derep.size2.uchime.fasta --centroids raw.qc.vsearch.fasta --fasta_width 0 -id 0.97 --sizein --uc raw.qc.uc.txt

# Convert the UC file to an OTU map file.
python $FAST/fast.py -parse_uc_cluster -i raw.qc.uc.txt -o raw.qc.vsearch.txt

# Merge OTU maps and generate FAST style hybrid OTU map
# Combine the derep FASTA and map
python $FAST/fast.py -generate_fast_map -map raw.qc.derep.size2.txt -seq raw.qc.derep.size2.uchime.fasta -o fast.derep.txt -derep

# Combine the OTU FASTA and map
python $FAST/fast.py -generate_fast_map -map raw.qc.vsearch.txt -seq raw.qc.vsearch.fasta -o fast.otu.txt -otu

# Combine the FAST derep and OTU map for a hybrid one
python $FAST/fast.py -combine_fast_map -derep_map fast.derep.txt -otu_map fast.otu.txt -o fast.hybrid.txt

# Use the hybrid map, rename OTUs
python $FAST/fast.py -rename_otu_map -fast_map fast.hybrid.txt -o fast.hybrid.otu.txt

# Generate the OTU table, and the representative sequences
python $FAST/fast.py -make_otu_table -fast_map fast.hybrid.otu.txt -o otu_table.txt -rep rep_seq.fasta

# Rarefy the OTU table to certain depth with iterations
# This script will rarefy each sample for a certain iterations (for example 1000 times), and pick the rarefied sample with the medium richness.
python $FAST/fast.py -rarefy_otu_table -otu otu_table.txt -o otu.table.rare.txt -d 20000 -iter 1000 -t 4


# SECTION-3 TAXONOMIC ASSIGNMENT
# Assign taxonomy
# Copy the BLAST database to your working folder,
# For Kinkel lab, the three files can be found under /home/kinkelll/shared/Tools
# 	unite_filter.nhr
#	unite_filter.nin
#	unite_filter.nsq

# You can also build your own BLAST database, and filer it by removing "unidentified" fungi.
# You can download the latest UNITE database from their website.
#python $FAST/filter_database -i sh_general_release_s_02.03.2015.fasta -o unite_filter.fasta
#makeblastdb -in unite_filter.fasta -dbtype nucl -out unite_filter

# Blast your representative sequences against your BLAST database
blastn -db unite_filter -query fungi_rep_set.fasta -max_target_seqs 1 -outfmt "6 qseqid stitle qlen length pident evalue" -out rep.otu.txt

# Map BLAST result to the OTU table
python $FAST/assign_taxonomy.py -otu fungi_otu_table.txt -tax rep.otu.txt -o fungi_otu_table.tax.txt

# Filter low mathc records
# I haven’t come up with a script here (will do in the future), but for now it is easy and flexible to do this in EXCEL. Filter out any rows that below a threshold of percentage match length (Subject_Len/Query_Len, such as 0.85), and/or below a threshold of Pident (such as 85).
# I usualy apply this filtering to all sequences that are less than 1% of total sequences, knowing that the abundant OTU may be some unknown fungi that do not map to the database.

#!!Remeber to remove all BLAST column except the taxonomy after this step.
# In this example, I save the new OTU table as fungi_otu_table.tax.filtered.txt

# Subtract Negative Control
# Subtract the abundance of negative control (if you have one) from other samples in EXCEL, or remove any OTU if you think it is necessary.

# Rarefaction. You will need to random sample your OTU table to a certain depth. Using the FAST package, you will be able to do iterated rarefaction, and pick a representative subsample.
# "-keep_all" will keep all samples even for those that are below the depth.
# "-d" is the seuquencing depth
# "-iter" sets the iteration number, 1000 is a good one.
# "-thread" set the number of CPU to be used.
python $FAST/rarefy_otu_table.py -otu fungi_otu_table.tax.filtered.txt -o fungi_otu_table.rare.txt -d 15000 -iter 1000 -thread 4 -keep_all -meta_column taxonomy

# The output OTU table is a text file (not BIOM format like in QIIME) and should be ready for any analysis.